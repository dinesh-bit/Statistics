{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a517ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in c:\\users\\chethan mk\\anaconda3\\lib\\site-packages (0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2txt\n",
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56d71dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirement = docx2txt.process(r'C:\\Users\\chethan mk\\Downloads\\NLP\\requirement.docx')\n",
    "resume = docx2txt.process(r'C:\\Users\\chethan mk\\Downloads\\NLP\\nlpresume.docx')\n",
    "#resume = docx2txt.process(r'C:\\Users\\chethan mk\\Downloads\\NLP\\nlpresume2.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0f1d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chethan MK\n",
      "\n",
      "Data Scientist\n",
      "\n",
      "84978000266\n",
      "\n",
      "chethanmudbasallar@gmail.com\n",
      "\n",
      "\n",
      "\n",
      "Carrier: Have 6 months of experience in Data Science. Have experience in building complete end-to-end Machine Learning models\n",
      "\n",
      "Skills :\n",
      "\n",
      "Machine Learning Algorithms\n",
      "\n",
      "Statistics and Maths\n",
      "\n",
      "Feature Engineering\n",
      "\n",
      "Data Analysis\n",
      "\n",
      "SQL\n",
      "\n",
      "Pandas, PySpark, H2O (Data Frame )\n"
     ]
    }
   ],
   "source": [
    "print(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab5ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi All,\n",
      "\n",
      "We are looking for a Machine Learning Engineer, location is in Bengaluru.\n",
      "\n",
      "Required Skills:\n",
      "\n",
      "Data analysis \n",
      "\n",
      "Feature engineering\n",
      "\n",
      "Metrics involved in Machine Learning\n",
      "\n",
      "Maths and Statistics\n",
      "\n",
      "Roles and Responsibility:\n",
      "\n",
      "Plays the main role in deciding and selecting machine learning libraries for a given task\n",
      "\n",
      "Analysis and checks the suitability of an algorithm\n"
     ]
    }
   ],
   "source": [
    "print(requirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b077c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\chethan\n",
      "[nltk_data]     mk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611bf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ws = WordNetLemmatizer()\n",
    "words = ws.lemmatize(requirement)\n",
    "#words = nltk.word_tokenize(requirement)\n",
    "words = re.findall(r'\\w+', requirement)\n",
    "words = list(filter(lambda sub: sub.isalpha(), words))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_requirement = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_requirement.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e5809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'All',\n",
       " 'We',\n",
       " 'looking',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Engineer',\n",
       " 'location',\n",
       " 'Bengaluru',\n",
       " 'Required',\n",
       " 'Skills',\n",
       " 'Data',\n",
       " 'analysis',\n",
       " 'Feature',\n",
       " 'engineering',\n",
       " 'Metrics',\n",
       " 'involved',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Maths',\n",
       " 'Statistics',\n",
       " 'Roles',\n",
       " 'Responsibility',\n",
       " 'Plays',\n",
       " 'main',\n",
       " 'role',\n",
       " 'deciding',\n",
       " 'selecting',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'libraries',\n",
       " 'given',\n",
       " 'task',\n",
       " 'Analysis',\n",
       " 'checks',\n",
       " 'suitability',\n",
       " 'algorithm']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbac173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sentences = nltk.sent_tokenize(resume)\n",
    "# # print(sentences)\n",
    "# sentences = re.findall(r'\\w+', resume)\n",
    "# #sentences = re.findall(r'^[a-zA-z]', resume)\n",
    "# print(sentences)\n",
    "# sentences = list(filter(lambda sub: sub.isalpha(), sentences))\n",
    "# print(sentences)\n",
    "# # for senw in sentences:\n",
    "# #     words = nltk.word_tokenize(senw)\n",
    "# #     print(words)\n",
    "# # for i in range (len(sentences)):\n",
    "# #     words = nltk.word_tokenize(sentences[i])\n",
    "# #     print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6384007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = re.findall(r'\\w+', resume)\n",
    "# words\n",
    "# words = list(filter(lambda sub: sub.isalpha(), sentences))\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9df06073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4395e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = WordNetLemmatizer()\n",
    "resume = resume.lower()\n",
    "\n",
    "# sentences = re.findall(r'\\w+', words)\n",
    "# sentences = list(filter(lambda sub: sub.isalpha(), sentences))\n",
    "words = ws.lemmatize(resume)\n",
    "#words = nltk.word_tokenize(resume)\n",
    "words = re.findall(r'\\w+', resume)\n",
    "words = list(filter(lambda sub: sub.isalpha(), words))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_resume = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_resume.append(w)\n",
    "# print(sentences)\n",
    "# corpus = []\n",
    "# for i in range(len(words)):\n",
    "#     res = words[i]\n",
    "#     res = res.lower()\n",
    "#     res = res.split(\" \")\n",
    "#     res = [ws.lemmatize(word) for word in res if not word in set(stopwords.words('english'))]\n",
    "#     res = ' '.join(res)\n",
    "#     corpus.append(res)\n",
    "\n",
    "# sentences = nltk.sent_tokenize(resume)\n",
    "# for i in range(len(sentences)):\n",
    "#     words = nltk.word_tokenize(sentences[i])\n",
    "# #words = nltk.word_tokenize(sentences)\n",
    "# corpus = []\n",
    "# for i in range(len(words)):\n",
    "#     res = [sub for sub in words[i] if sub.isalpha()]\n",
    "# #     res = re.sub('[^a-zA-Z]',\"\", words[i])\n",
    "# #     res = res.lower()\n",
    "# #     res = res.split(\" \")\n",
    "#     res = [ws.lemmatize(word) for word in res if not word in set(stopwords.words('english'))]\n",
    "#     res = ' '.join(res)\n",
    "#     corpus.append(res)\n",
    "# # print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c48c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr: ['chethan', 'mk', 'data', 'scientist', 'chethanmudbasallar', 'gmail', 'com', 'carrier', 'months', 'experience', 'data', 'science', 'experience', 'building', 'complete', 'end', 'end', 'machine', 'learning', 'models', 'skills', 'machine', 'learning', 'algorithms', 'statistics', 'maths', 'feature', 'engineering', 'data', 'analysis', 'sql', 'pandas', 'pyspark', 'data', 'frame']\n",
      "fr: ['Hi', 'All', 'We', 'looking', 'Machine', 'Learning', 'Engineer', 'location', 'Bengaluru', 'Required', 'Skills', 'Data', 'analysis', 'Feature', 'engineering', 'Metrics', 'involved', 'Machine', 'Learning', 'Maths', 'Statistics', 'Roles', 'Responsibility', 'Plays', 'main', 'role', 'deciding', 'selecting', 'machine', 'learning', 'libraries', 'given', 'task', 'Analysis', 'checks', 'suitability', 'algorithm']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"fr:\",filtered_resume)\n",
    "print(\"fr:\",filtered_requirement)\n",
    "print(type(filtered_resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "057739af",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirement = requirement.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a536cefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi all,\\n\\nwe are looking for a machine learning engineer, location is in bengaluru.\\n\\nrequired skills:\\n\\ndata analysis \\n\\nfeature engineering\\n\\nmetrics involved in machine learning\\n\\nmaths and statistics\\n\\nroles and responsibility:\\n\\nplays the main role in deciding and selecting machine learning libraries for a given task\\n\\nanalysis and checks the suitability of an algorithm'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "972e9cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "715ec7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity:  0.08585457105482137\n"
     ]
    }
   ],
   "source": [
    "X_list = word_tokenize(requirement) \n",
    "Y_list = word_tokenize(resume)\n",
    "  \n",
    "# sw contains the list of stopwords\n",
    "sw = stopwords.words('english') \n",
    "l1 =[];l2 =[]\n",
    "  \n",
    "# remove stop words from the string\n",
    "X_set = {w for w in X_list if not w in sw} \n",
    "Y_set = {w for w in Y_list if not w in sw}\n",
    "  \n",
    "# form a set containing keywords of both strings \n",
    "rvector = X_set.union(Y_set) \n",
    "for w in rvector:\n",
    "    if w in X_set: l1.append(1) # create a vector\n",
    "    else: l1.append(0)\n",
    "    if w in Y_set: l2.append(1)\n",
    "    else: l2.append(0)\n",
    "c = 0\n",
    "  \n",
    "# cosine formula \n",
    "for i in range(len(rvector)):\n",
    "        c+= l1[i]*l2[i]\n",
    "cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "print(\"similarity: \", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e5391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adb3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(filtered_requirement)\n",
    "cm = cv.transform(filtered_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_matrix)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f4fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim = cosine_similarity(count_matrix, cm)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e96970",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirement = requirement.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60975c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = resume.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(requirement)\n",
    "cm = cv.transform(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cabd90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "count_matrix = tv.fit_transform(corpusreq)\n",
    "cm = tv.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpusreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a92267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "sim = cosine_similarity(count_matrix, cm)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3c8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "cosine_distances(count_matrix, cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fa0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48226412",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac68fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553c901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = requirement.lower()\n",
    "s1 = requirement.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee256aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    return [i.lower() for i in s.split()]\n",
    "\n",
    "def get_vector(s):\n",
    "    return np.sum(np.array([model[i] for i in preprocess(s)]), axis=0)\n",
    "\n",
    "from scipy import spatial\n",
    "print('s0 vs s1 ->',1 - spatial.distance.cosine(get_vector(s0), get_vector(s1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
